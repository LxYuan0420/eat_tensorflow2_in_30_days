{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-2 Feature Column.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2YkXFgHcTf/Dn4tOb+XuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/5_2_Feature_Column.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTlqc7XVdqAA"
      },
      "source": [
        "**5-2 feature_column**\r\n",
        "\r\n",
        "Feature column is usually applied in the feature engineering for the structured data, while rarely used for the image or text date.\r\n",
        "\r\n",
        "**1. Introduction about how to use feature column**\r\n",
        "\r\n",
        "Feature column is used to converting category features into one-hot encoding, or creating bucketing feature from continuous feature, or generating cross features from multiple features, etc.\r\n",
        "\r\n",
        "Before creating feature column, please call the functions in the module tf.feature_column. The nine most frequently used functions in this module are shown in the figure below. All these functions will return a Categorical-Column or a Dense-Column object, but will not return bucketized_column, since the last class is inhereted from the first two classes.\r\n",
        "\r\n",
        "Be careful: all the Categorical-Column class have to be converted into Dense-Column class through indicator_column before input to the model.\r\n",
        "\r\n",
        "- `numeric_column`, the most frequently used function.\r\n",
        "\r\n",
        "- `bucketized_column`, generated from numerical column, listing multiple features from a numerical clumn; it is one-hot encoded.\r\n",
        "\r\n",
        "- `categorical_column_with_identity`, one-hot encoded, identical to the case that each bucket is one interger.\r\n",
        "\r\n",
        "- `categorical_column_with_vocabulary_list`, one-hot encoded; the dictionary is specified by the list.\r\n",
        "\r\n",
        "- `categorical_column_with_vocabulary_file`ï¼Œ one-hot encoded; the dictionary is specified by the file.\r\n",
        "\r\n",
        "- `categorical_column_with_hash_bucket`, used in the case with a large interger or a large dictionary.\r\n",
        "\r\n",
        "- `indicator_column`, generated by Categorical-Column; one-hot encoded.\r\n",
        "\r\n",
        "- `embedding_column`, generated by Categorical Column; the embedded vector distributed parameter needs learning/training. The recommended dimension of the embedded vector is the fourth root to the number of categories.\r\n",
        "\r\n",
        "- `crossed_column`, consists of arbitrary category column except for categorical_column_with_hash_bucket\r\n",
        "\r\n",
        "**2. Demonstration of feature column**\r\n",
        "Here is a complete example that solves Titanic survival problmen using feature column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nl5lLJHg_1c"
      },
      "source": [
        "import datetime\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers,models\r\n",
        "\r\n",
        "\r\n",
        "# Printing log\r\n",
        "def printlog(info):\r\n",
        "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n",
        "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\r\n",
        "    print(info+'...\\n\\n')\r\n",
        "\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NHG_T5YhLGd"
      },
      "source": [
        "**1. Constructing data pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbmnMmGChQ5z"
      },
      "source": [
        "dftrain_raw = pd.read_csv(\"../data/titanic/train.csv\")\r\n",
        "dftest_raw = pd.read_csv(\"../data/titanic/test.csv\")\r\n",
        "\r\n",
        "dfraw = pd.concat([dftrain_raw, dftest_raw])\r\n",
        "\r\n",
        "def prepare_dfdata(dfraw):\r\n",
        "    dfdata = dfraw.copy()\r\n",
        "    dfdata.columns = [x.lower() for x in dfdata.columns]\r\n",
        "    dfdata = dfdata.rename(columns={\"survived\": \"label\"})\r\n",
        "    dfdata.drop([\"passengerid\", \"name\"], axis=1, inplace=True)\r\n",
        "\r\n",
        "    for col, dtype in dict(dfdata.dtypes).items():\r\n",
        "        #see if there are missing values.\r\n",
        "        if dfdata[col].hasnans:\r\n",
        "            dfdata[col + \"_nan\"] = pd.isna(dfdata[col]).astype('int32')\r\n",
        "\r\n",
        "            if dtype not in [np.object, np.str, np.unicode]:\r\n",
        "                dfdata[col].fillna(dfdata[col].mean(), inplace=True)\r\n",
        "            else:\r\n",
        "                dfdata[col].fillna('', inplace=True)\r\n",
        "    return (dfdata)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "dfdata = prepare_dfdata(dfraw)\r\n",
        "dftrain = dfdata.iloc[0:len(dftrain_raw),:]\r\n",
        "dftest = dfdata.iloc[len(dftrain_raw):,:]\r\n",
        "\r\n",
        "# importing data from dataframe   \r\n",
        "# Importing data from dataframe\r\n",
        "def df_to_dataset(df, shuffle=True, batch_size=32):\r\n",
        "    dfdata = df.copy()\r\n",
        "    if 'label' not in dfdata.columns:\r\n",
        "        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = 'list'))\r\n",
        "    else: \r\n",
        "        labels = dfdata.pop('label')\r\n",
        "        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = 'list'), labels))  \r\n",
        "    if shuffle:\r\n",
        "        ds = ds.shuffle(buffer_size=len(dfdata))\r\n",
        "    ds = ds.batch(batch_size)\r\n",
        "    return ds\r\n",
        "\r\n",
        "ds_train = df_to_dataset(dftrain)\r\n",
        "ds_test = df_to_dataset(dftest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkggVEziviX"
      },
      "source": [
        "#================================================================================\r\n",
        "# 2. Defining the feature column\r\n",
        "#================================================================================\r\n",
        "printlog(\"step2: make feature columns...\")\r\n",
        "\r\n",
        "feature_columns = []\r\n",
        "\r\n",
        "# Numerical column\r\n",
        "for col in ['age','fare','parch','sibsp'] + [\r\n",
        "    c for c in dfdata.columns if c.endswith('_nan')]:\r\n",
        "    feature_columns.append(tf.feature_column.numeric_column(col))\r\n",
        "\r\n",
        "# Bucketized column\r\n",
        "age = tf.feature_column.numeric_column('age')\r\n",
        "age_buckets = tf.feature_column.bucketized_column(age, \r\n",
        "             boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\r\n",
        "feature_columns.append(age_buckets)\r\n",
        "\r\n",
        "# Category column\r\n",
        "# NOTE: all the Categorical-Column class have to be converted into Dense-Column class through `indicator_column` before input to the model.\r\n",
        "sex = tf.feature_column.indicator_column(\r\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(\r\n",
        "      key='sex',vocabulary_list=[\"male\", \"female\"]))\r\n",
        "feature_columns.append(sex)\r\n",
        "\r\n",
        "pclass = tf.feature_column.indicator_column(\r\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(\r\n",
        "      key='pclass',vocabulary_list=[1,2,3]))\r\n",
        "feature_columns.append(pclass)\r\n",
        "\r\n",
        "ticket = tf.feature_column.indicator_column(\r\n",
        "     tf.feature_column.categorical_column_with_hash_bucket('ticket',3))\r\n",
        "feature_columns.append(ticket)\r\n",
        "\r\n",
        "embarked = tf.feature_column.indicator_column(\r\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(\r\n",
        "      key='embarked',vocabulary_list=['S','C','B']))\r\n",
        "feature_columns.append(embarked)\r\n",
        "\r\n",
        "# Embedding column\r\n",
        "cabin = tf.feature_column.embedding_column(\r\n",
        "    tf.feature_column.categorical_column_with_hash_bucket('cabin',32),2)\r\n",
        "feature_columns.append(cabin)\r\n",
        "\r\n",
        "# Crossed column\r\n",
        "pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(\r\n",
        "          key='pclass',vocabulary_list=[1,2,3])\r\n",
        "\r\n",
        "crossed_feature = tf.feature_column.indicator_column(\r\n",
        "    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=15))\r\n",
        "\r\n",
        "feature_columns.append(crossed_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNIXN4rhixNc"
      },
      "source": [
        "#================================================================================\r\n",
        "# 3. Defining the model\r\n",
        "#================================================================================\r\n",
        "printlog(\"step3: define model...\")\r\n",
        "\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "model = tf.keras.Sequential([\r\n",
        "  layers.DenseFeatures(feature_columns), # Placing the feature into tf.keras.layers.DenseFeatures\r\n",
        "  layers.Dense(64, activation='relu'),\r\n",
        "  layers.Dense(64, activation='relu'),\r\n",
        "  layers.Dense(1, activation='sigmoid')\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ij5hEJei4iG"
      },
      "source": [
        "#================================================================================\r\n",
        "# 4. Training the model\r\n",
        "#================================================================================\r\n",
        "printlog(\"step4: train model...\")\r\n",
        "\r\n",
        "model.compile(optimizer='adam',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "history = model.fit(ds_train,\r\n",
        "          validation_data=ds_test,\r\n",
        "          epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCkJ5j-gi5c1"
      },
      "source": [
        "#================================================================================\r\n",
        "# 5. Evaluating the model\r\n",
        "#================================================================================\r\n",
        "printlog(\"step5: eval model...\")\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "%config InlineBackend.figure_format = 'svg'\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def plot_metric(history, metric):\r\n",
        "    train_metrics = history.history[metric]\r\n",
        "    val_metrics = history.history['val_'+metric]\r\n",
        "    epochs = range(1, len(train_metrics) + 1)\r\n",
        "    plt.plot(epochs, train_metrics, 'bo--')\r\n",
        "    plt.plot(epochs, val_metrics, 'ro-')\r\n",
        "    plt.title('Training and validation '+ metric)\r\n",
        "    plt.xlabel(\"Epochs\")\r\n",
        "    plt.ylabel(metric)\r\n",
        "    plt.legend([\"train_\"+metric, 'val_'+metric])\r\n",
        "    plt.show()\r\n",
        "\r\n",
        "plot_metric(history,\"accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}