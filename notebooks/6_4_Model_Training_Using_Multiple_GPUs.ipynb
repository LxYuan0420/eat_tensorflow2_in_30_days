{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-4 Model Training Using Multiple GPUs.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO8ef0KYMyz2ieAPg/Zv6U8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/6_4_Model_Training_Using_Multiple_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30mzHPlJ-isT"
      },
      "source": [
        "**6-4 Model Training Using Multiple GPUs**\r\n",
        "\r\n",
        "We recommend using pre-defined fit method for training when using multiple GPU, which only requires two additional lines of code.\r\n",
        "\r\n",
        "In Colab notebook, choose GPU in Edit -> Notebook Settings -> Hardware Accelerator\r\n",
        "\r\n",
        "Note: the following code only executes on Colab.\r\n",
        "\r\n",
        "You may use the following link for testing (tf_multiGPU, in Chinese)\r\n",
        "\r\n",
        "https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-\r\n",
        "\r\n",
        "Introduction to MirroredStrategyï¼š\r\n",
        "\r\n",
        "- The strategy gives a copy to each of the N computing devices before training.\r\n",
        "- When a batch of training data is received, devide the data into N portions and transfer them into N devices (data parallelism)\r\n",
        "- Each device calculate the local variables (mirrored variables) to calculate the gradient according to the received portion of data.\r\n",
        "- Implement All-reduce operation in parallel computing, exchange the gradient data and calculate summation, allows each device to obtain the gradient sum from all the devices.\r\n",
        "- Update the local variables (mirrored variables) using the gradient sum.\r\n",
        "- Proceed to the next round of training when all the devices updated their local variables (This is a fully synchronized strategy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "41_1dYpE-o4H",
        "outputId": "7c10f514-69cb-4a1b-e444-ee3dd44c624f"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPcQK3l_GrH"
      },
      "source": [
        "from tensorflow.keras import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tmydrpg_I7e",
        "outputId": "7b044805-04e7-428e-9d6c-31890c8cce19"
      },
      "source": [
        "#Simulate two logical GPUs with one physical GPU\r\n",
        "\r\n",
        "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\n",
        "\r\n",
        "if gpus:\r\n",
        "    try: \r\n",
        "        tf.config.experimental.set_virtual_device_configuration(gpus[0],\r\n",
        "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\r\n",
        "             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\r\n",
        "        \r\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n",
        "        print(f\"{len(gpus)} Physical GPU {len(logical_gpus)} logical gpus\")\r\n",
        "\r\n",
        "    except RuntimeError as e:\r\n",
        "        print(e)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPU 2 logical gpus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvRl-e6_1FS"
      },
      "source": [
        "**1. Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_equEFNJ_67t",
        "outputId": "0f69042d-0165-45bf-d720-aa0d809e6f36"
      },
      "source": [
        "MAX_LEN = 300\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = datasets.reuters.load_data()\r\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\r\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_LEN)\r\n",
        "\r\n",
        "MAX_WORDS = x_train.max() + 1\r\n",
        "CAT_NUM = y_train.max() + 1\r\n",
        "\r\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE).cache()\r\n",
        "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE).cache()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPnTI3cOAlNt"
      },
      "source": [
        "def create_model():\r\n",
        "\r\n",
        "    model = models.Sequential()\r\n",
        "    model.add(layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN))\r\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=5, activation='relu'))\r\n",
        "    model.add(layers.MaxPool1D())\r\n",
        "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\r\n",
        "    model.add(layers.MaxPool1D())\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(CAT_NUM, activation='sigmoid'))\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "def compile_model(model):\r\n",
        "    model.compile(\r\n",
        "        loss = losses.SparseCategoricalCrossentropy(),\r\n",
        "        optimizer = optimizers.Nadam(),\r\n",
        "        metrics = [metrics.SparseCategoricalAccuracy(), metrics.SparseTopKCategoricalAccuracy()]\r\n",
        "    )\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WH1vyzhBTCh"
      },
      "source": [
        "**3. Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpaR5O2RBWjU",
        "outputId": "946b8801-98f8-4690-8eeb-82f33ed92945"
      },
      "source": [
        "# Add the following two lines of code\r\n",
        "strategy = tf.distribute.MirroredStrategy()\r\n",
        "with strategy.scope():\r\n",
        "    model = create_model()\r\n",
        "    model.summary()\r\n",
        "    model = compile_model(model)\r\n",
        "\r\n",
        "history = model.fit(ds_train, validation_data=ds_test, epochs=10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "281/281 [==============================] - 15s 19ms/step - loss: 2.4025 - sparse_categorical_accuracy: 0.3870 - sparse_top_k_categorical_accuracy: 0.7055 - val_loss: 1.6592 - val_sparse_categorical_accuracy: 0.5619 - val_sparse_top_k_categorical_accuracy: 0.7640\n",
            "Epoch 2/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 1.5506 - sparse_categorical_accuracy: 0.5994 - sparse_top_k_categorical_accuracy: 0.7832 - val_loss: 1.5010 - val_sparse_categorical_accuracy: 0.6264 - val_sparse_top_k_categorical_accuracy: 0.8054\n",
            "Epoch 3/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 1.2503 - sparse_categorical_accuracy: 0.6698 - sparse_top_k_categorical_accuracy: 0.8425 - val_loss: 1.5182 - val_sparse_categorical_accuracy: 0.6478 - val_sparse_top_k_categorical_accuracy: 0.8117\n",
            "Epoch 4/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.9842 - sparse_categorical_accuracy: 0.7413 - sparse_top_k_categorical_accuracy: 0.9009 - val_loss: 1.6872 - val_sparse_categorical_accuracy: 0.6447 - val_sparse_top_k_categorical_accuracy: 0.8081\n",
            "Epoch 5/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.7368 - sparse_categorical_accuracy: 0.8117 - sparse_top_k_categorical_accuracy: 0.9412 - val_loss: 1.9430 - val_sparse_categorical_accuracy: 0.6362 - val_sparse_top_k_categorical_accuracy: 0.8108\n",
            "Epoch 6/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.5471 - sparse_categorical_accuracy: 0.8631 - sparse_top_k_categorical_accuracy: 0.9676 - val_loss: 2.2144 - val_sparse_categorical_accuracy: 0.6233 - val_sparse_top_k_categorical_accuracy: 0.8001\n",
            "Epoch 7/10\n",
            "281/281 [==============================] - 4s 14ms/step - loss: 0.4320 - sparse_categorical_accuracy: 0.8996 - sparse_top_k_categorical_accuracy: 0.9774 - val_loss: 2.4583 - val_sparse_categorical_accuracy: 0.6207 - val_sparse_top_k_categorical_accuracy: 0.7983\n",
            "Epoch 8/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.3628 - sparse_categorical_accuracy: 0.9160 - sparse_top_k_categorical_accuracy: 0.9831 - val_loss: 2.6502 - val_sparse_categorical_accuracy: 0.6149 - val_sparse_top_k_categorical_accuracy: 0.8019\n",
            "Epoch 9/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.3156 - sparse_categorical_accuracy: 0.9261 - sparse_top_k_categorical_accuracy: 0.9889 - val_loss: 2.8486 - val_sparse_categorical_accuracy: 0.6095 - val_sparse_top_k_categorical_accuracy: 0.8023\n",
            "Epoch 10/10\n",
            "281/281 [==============================] - 4s 13ms/step - loss: 0.2825 - sparse_categorical_accuracy: 0.9322 - sparse_top_k_categorical_accuracy: 0.9918 - val_loss: 3.0293 - val_sparse_categorical_accuracy: 0.6082 - val_sparse_top_k_categorical_accuracy: 0.8023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS6FZVJABzhM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}