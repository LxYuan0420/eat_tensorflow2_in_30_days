{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-3 activation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTPvJ5QQDR3UpOD44ex787",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/5_3_activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Wqq67SjXzG"
      },
      "source": [
        "Activation function plays a key role in deep learning. It introduces the nonlinearity that enables the neural network to fit arbitrary complicated functions.\r\n",
        "\r\n",
        "The neural network, no matter how complicated the structure is, is still a linear transformation which cant fit the nonlinear functions without the activation function.\r\n",
        "\r\n",
        "For the time being, the most popular activation function is `relu`, but there are some new functions such as `swish`, `GELU`, claming a better performance over `relu`.\r\n",
        "\r\n",
        "Here are two review papers to the activation function (in Chinese).\r\n",
        "\r\n",
        "1. https://zhuanlan.zhihu.com/p/98472075\r\n",
        "2. https://zhuanlan.zhihu.com/p/98863801\r\n",
        "\r\n",
        "**1. The most popular activation functions**\r\n",
        "\r\n",
        "`tf.nn.sigmoid`: compressing real number between 0 to 1, usually used in the output layer for binary classification; the main drawbacks are vanishing gradient, high computing complexity, and the non-zero center of the output\r\n",
        "\r\n",
        "`tf.nn.softmax`: Extended version of sigmoid for multiple categories, usually used in the output layer for multiple classification.\r\n",
        "\r\n",
        "`tf.nn.tanh`: Compressing real number between -1 to 1, expectation of the output is zero; the main drawbacks are vanishing gradient and high computing complexity.\r\n",
        "\r\n",
        "`tf.nn.relu`: Linear rectified unit, the most popular activation function , usually used in the hidden layer; the main drawbacks are non-zero center of the output and vanishing gradient for the inputs <0 (dying relu).\r\n",
        "\r\n",
        "`tf.nn.leaky_relu`: Improved ReLU, resolving the dying relu problem.\r\n",
        "\r\n",
        "`tf.nn.elu`: Exponential linear unit, which is an improvement to the ReLU, alleviate the dying ReLU problem.\r\n",
        "\r\n",
        "`tf.nn.selu`: Scaled exponential linear unit, which is able to normalize the neural network automatically if the weights are initialized through `tf.keras.initializers.lecun_normal`. No gradient exploding/vanishing problems, but need to apply together with AlphaDropout (an alternation of Dropout)\r\n",
        "\r\n",
        "`tf.nn.swish`: Self-gated activation function, a research product from Google. The literature prove that it brings slight improvement comparing to ReLU.\r\n",
        "\r\n",
        "`gelu`: Gassion error linear unit, which has the best performance in Transformer; however, `tf.nn` hasn't implemented it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10iALSq4nUjd"
      },
      "source": [
        "**2. Implementing activation function in the models**\r\n",
        "\r\n",
        "There are two ways of implementing activation functions in keras models; specifiying through the `activation` parameter in certain layers, or adding activaion layer `layers.Activation` explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPs9Vt1lnqPq",
        "outputId": "31bcd885-bac8-477a-8f81-42107ba2a3f0"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "model = tf.keras.models.Sequential()\r\n",
        "model.add(tf.keras.layers.Dense(32, input_shape=(None, 16), activation=tf.nn.relu)) #specifiying through the activation parameter\r\n",
        "model.add(tf.keras.layers.Dense(10))\r\n",
        "model.add(tf.keras.layers.Activation(tf.nn.softmax)) #specifiying explicity\r\n",
        "model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, None, 32)          544       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 10)          330       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, None, 10)          0         \n",
            "=================================================================\n",
            "Total params: 874\n",
            "Trainable params: 874\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyCu9tqNpL2F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}