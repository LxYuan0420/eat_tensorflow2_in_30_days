{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5-7 optimizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuSzwv3rNa0fIUsyO4mVmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/5_7_optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7GuBzZF_dL0"
      },
      "source": [
        "**5-7 optimizers**\r\n",
        "\r\n",
        "There is a group of magic cooks in machine learning. Their daily life looks like:\r\n",
        "\r\n",
        "They grab some raw material (data), put them into a pot (model), light some fire (optimization algorithm), and wait until the cuisine is ready.\r\n",
        "\r\n",
        "However, anyone who has cooking experience knows that fire controlling is the key part. Even using same material with the same recipe, different fire level leads to totally different results: medium well, burnt, or still raw.\r\n",
        "\r\n",
        "This theroy on cooking also applies to the machine learning. The choice of the optimization algorithm determines the final performance of the final model. An unsatisfying performance is not necessarily due to the problem of feature or model designing, instead, it might be attributed to the choice of optimization algorithm.\r\n",
        "\r\n",
        "The evolution of the optimization algorithm for the deep learning is: SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam\r\n",
        "\r\n",
        "You may refer to the following article to for more details \"Understand the differences in optimization algorthms with just one framework: SGD/AdaGrad/Adam\"\r\n",
        "\r\n",
        "For the beginners, choosing Adam as the optimizer and using the default parameters will set everything for you.\r\n",
        "\r\n",
        "Some researchers who are chaising better metrics for publications could use Adam as the initial optimizer and use SGD later for fine-tuning the parameters for better performance.\r\n",
        "\r\n",
        "There are some cutting-edge optimization algorithms claiming a better performance, e.g. LazyAdam, Look-ahead, RAdam, Ranger, etc.\r\n",
        "\r\n",
        "**1. How To Use the Optimizer**\r\n",
        "\r\n",
        "Optimizer accepts variables and corresponding gradient through apply_gradients method to iterate over the given variables. Another way is using minimize method to optimize the target function iteratively.\r\n",
        "\r\n",
        "Another common way is passing the optimizer to the Model of keras, and call model.fit method to optimize the loss function.\r\n",
        "\r\n",
        "A variable named optimizer.iterations will be created during optimizer initialization to record the number of iteration. Thus the optimizer should be created outside the decorator @tf.function with the same reason as tf.Variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdwq5Q91DCVL"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np \r\n",
        "\r\n",
        "# Time stamp\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    ts = tf.timestamp()\r\n",
        "    today_ts = ts%(24*60*60)\r\n",
        "\r\n",
        "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\r\n",
        "    minite = tf.cast((today_ts%3600)//60,tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\r\n",
        "    \r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\r\n",
        "            return(tf.strings.format(\"0{}\",m))\r\n",
        "        else:\r\n",
        "            return(tf.strings.format(\"{}\",m))\r\n",
        "    \r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8,end = \"\")\r\n",
        "    tf.print(timestring)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv0LXEOdDH5r",
        "outputId": "1f551209-fef4-46b0-a37f-95c17a1d51db"
      },
      "source": [
        "# the minial value of f(x) = a*x**2 + b*x + c\r\n",
        "# Here we use optimizer.apply_gradients\r\n",
        "\r\n",
        "x = tf.Variable(0.0, name='x', dtype=tf.float32)\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def minimizef():\r\n",
        "    a = tf.constant(2.0)\r\n",
        "    b = tf.constant(-2.0)\r\n",
        "    c = tf.constant(1.0)\r\n",
        "\r\n",
        "    while tf.constant(True):\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            y = a*tf.pow(x, 2) + b*c + c\r\n",
        "        dy_dx = tape.gradient(y, x)\r\n",
        "        optimizer.apply_gradients(grads_and_vars=[(dy_dx, x)])\r\n",
        "\r\n",
        "        # Condition of terminating the iteration\r\n",
        "        if tf.abs(dy_dx)<tf.constant(0.00001):\r\n",
        "            break\r\n",
        "            \r\n",
        "        if tf.math.mod(optimizer.iterations,100)==0:\r\n",
        "            printbar()\r\n",
        "            tf.print(\"step = \",optimizer.iterations)\r\n",
        "            tf.print(\"x = \", x)\r\n",
        "            tf.print(\"\")\r\n",
        "                \r\n",
        "    y = a*tf.pow(x,2) + b*x + c\r\n",
        "    return y\r\n",
        "\r\n",
        "tf.print(\"y =\",minimizef())\r\n",
        "tf.print(\"x =\",x)       "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 1\n",
            "x = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKPym_T3HUvs",
        "outputId": "9c645c0c-27d7-4dd8-e475-5d1fdd4c26b1"
      },
      "source": [
        "# Minimal value of f(x) = a*x**2 + b*x + c\r\n",
        "# Here we use optimizer.minimize\r\n",
        "\r\n",
        "x = tf.Variable(0.0, name='x', dtype=tf.float32)\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n",
        "\r\n",
        "def f():\r\n",
        "    a = tf.constant(2.0)\r\n",
        "    b = tf.constant(-2.0)\r\n",
        "    c = tf.constant(1.0)\r\n",
        "    y = a*tf.pow(x,2) + b*x + c\r\n",
        "    return y\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train(epochs=100):\r\n",
        "    for _ in tf.range(epochs):\r\n",
        "        optimizer.minimize(f, [x])\r\n",
        "    tf.print(\"epoch = \", optimizer.iterations)\r\n",
        "    return (f())\r\n",
        "\r\n",
        "train(1000)\r\n",
        "tf.print(\"y = \", f())\r\n",
        "tf.print(\"x = \", x) "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch =  1000\n",
            "y =  0.5\n",
            "x =  0.499999642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqCXYZ_nHU0D",
        "outputId": "1b4b5d87-230c-41eb-e070-bd0de1b3cfe7"
      },
      "source": [
        "# Minimal value of f(x) = a*x**2 + b*x + c\r\n",
        "# Here we use model.fit\r\n",
        "\r\n",
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "class FakeModel(tf.keras.models.Model):\r\n",
        "    def __init__(self,a,b,c):\r\n",
        "        super(FakeModel,self).__init__()\r\n",
        "        self.a = a\r\n",
        "        self.b = b\r\n",
        "        self.c = c\r\n",
        "    \r\n",
        "    def build(self):\r\n",
        "        self.x = tf.Variable(0.0,name = \"x\")\r\n",
        "        self.built = True\r\n",
        "    \r\n",
        "    def call(self,features):\r\n",
        "        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c\r\n",
        "        return(tf.ones_like(features)*loss)\r\n",
        "    \r\n",
        "def myloss(y_true,y_pred):\r\n",
        "    return tf.reduce_mean(y_pred)\r\n",
        "\r\n",
        "model = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))\r\n",
        "\r\n",
        "model.build()\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(optimizer = \r\n",
        "              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)\r\n",
        "history = model.fit(tf.zeros((100,2)),\r\n",
        "                    tf.ones(100),batch_size = 1,epochs = 10)  # Iterate for 1000 times"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"fake_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Total params: 1\n",
            "Trainable params: 1\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 0s 834us/step - loss: 0.4930\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 768us/step - loss: 0.0087\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 792us/step - loss: 1.5250e-04\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 806us/step - loss: 2.6838e-06\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 871us/step - loss: 4.3807e-08\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 756us/step - loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 813us/step - loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 794us/step - loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 782us/step - loss: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH4UlaUUHU2o",
        "outputId": "c0ed97ec-9604-4d6f-ffb1-8f87d0f0ef92"
      },
      "source": [
        "tf.print(\"x=\",model.x)\r\n",
        "tf.print(\"loss=\",model(tf.constant(0.0)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x= 0.99999851\n",
            "loss= 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6sGp4SXHU5x"
      },
      "source": [
        "**2. Pre-defined Optimizers**i\r\n",
        "\r\n",
        "The evolution of the optimization algorithm for the deep learning is: SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam\r\n",
        "\r\n",
        "There are corresponding classes in keras.optimizers sub-module as the implementations of these optimizers.\r\n",
        "\r\n",
        "- SGD, the default parameters is for a pure SGD. For a non-zero parameter momentum, the optimizer changes to SGDM since it considers the first-order momentum. For nesterov = True, the optimizer changes to NAG (Nesterov Accelerated Gradient), which calculates the gradient of the one further step.\r\n",
        "\r\n",
        "- - Adagrad, considers the second-order momentum and equipted with self-adaptive learning rate; the drawback is a slow learning rate at a later stage or early ceasing of learning due to the monotonically desending leanring rate.\r\n",
        "\r\n",
        "- RMSprop, considers the second-order momentum and equipted with self-adaptive learning rate; improves the Adagrad through exponential smoothing, which only cnosiders the second-order momentum in a given window length.\r\n",
        "\r\n",
        "- Adadelta, considers the second-order momentum, similar as RMSprop but more complicated with an improved self-adaption.\r\n",
        "\r\n",
        "- Adam, consider both the first-order and the second-order momentum; it improves RMSprop by including first-order momentum.\r\n",
        "\r\n",
        "- Nadam, improves Adam by including Nesterov Acceleration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z79_VEQMJyFj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}