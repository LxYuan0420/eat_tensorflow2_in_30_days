{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-3 Example: Modeling Procedure for Texts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNM1Mno4ljSQb93d/Jn48/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/1_3_Example_Modeling_Procedure_for_Texts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00_vWsqnh9BZ",
        "outputId": "1b2920a4-2ff8-4396-b611-5b001445e3e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toS1F19wk69p",
        "outputId": "b1d96f98-05dc-4e13-c15a-237ccec8eeef"
      },
      "source": [
        "%cd \"/gdrive/MyDrive/Colab Notebooks/git/eat_tensorflow2_in_30_days/notebooks\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/git/eat_tensorflow2_in_30_days/notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0QQjQP4loi5",
        "outputId": "7c9b707e-5e2e-49a4-ebdb-c4526a5e7cd8"
      },
      "source": [
        "!cat \"../data/imdb/test.csv\" | head -5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\tThe first one meant victory. This one means defeat. It takes place in a Bolivia, there the guerillas are sick and wary and don't meet that much sympathy from the farmers. If you know your 60s history, you understand how it ends. You will understand it even without that knowledge.<br /><br />Del Toro is once again splendid. He goes on building this icon about the revolutionary who remains the same, regardless of success or failure. That's what Guevara is according to the legend, but still it's so well acted.<br /><br />The documentary feeling is there around the icon, which is one of the greatest achievements in this big Soderbergh project. He has succeeded.\r\n",
            "1\tExcellent movie, a realistic picture of contemporary Finland, touching and profound. One of the best Finnish films ever made. Captures marvelously the everyday life in a Central Finland small town, people's desires and weaknesses, joys and sorrows. The bright early fall sunshine creates a cool atmosphere to this lucid examination of people in a welfare society. Lampela is indeed one of the most promising Finnish filmmakers. He shows that it is possible to make gripping movies without machine guns and bloodshed. His next film Eila is also worth seeing although the story of cleaning women fighting for their jobs is not quite as universally appealing as the destinies in Joki.\r\n",
            "1\tThis film is moving without being sentimental - meaningful without being pretentious. It tells a simple story of a family in danger of falling apart as the encroachments of technology and an advancing society make the family-run business increasingly untenable.<br /><br />The acting is wonderful - though none of us in the west are likely to have heard of these actors, we should have long ago - they play their characters with honesty and reverence - these are flawed characters, each with major weaknesses, but with such utter humanity and kindness that it's impossible not to become engaged in the story.<br /><br />We need more films like this - we need more western filmmakers creating films such as this.\r\n",
            "0\t\"This is high grade cheese fare of B movie kung fu flicks. Bruce \"\"wannabe\"\" Lee is played by Bruce Li...I think. Of course, let's show quick clips of Bruce and do closeups of his eyes and if you quint at the right angle during a certain time of the day during the winter solstice, it kind of looks like Bruce. You'll laugh in awe at how the film splicing isn't very good, but some cool deleted scenes from Enter the Dragon are thrown in the mix. According to the movie, Bruce Lee was killed by a dart while hanging from a helicopter. Of course, they think this can excuse Bruce Li for trying to be Bruce even though his character is supposed to be Bruce's brother (who for some reason still mimes Bruce's gestures and fighting style - very POORLY). See Bruce go one-on-one with the cowardly lion. The props department stopped by Kay-Bee, you see. Bruce also finds nothing wrong with savagely beating up a crippled man. Towards the end, the director decided \"\"let's throw a flashback\"\" for a scene just shown 3 minutes ago!! They must've thought that only one-celled organisms with attention deficit disorder could fully understand this film.<br /><br />\"\r\n",
            "1\tHalloween is the story of a boy who was misunderstood as a child. He takes out his problems on his older sister, whom he murders at the beginning of the film. This is just the start of things to come from Michael Myers.<br /><br />Donald Pleasance plays the doctor who's been studying Myers for years. He knows that something is different about him, something mysteriously evil. This evil will not be contained, and it cannot be stopped.<br /><br />After an escape from an institution, Myers tracks down his younger sister. If he kills her, there may be an end to the troubles of this misunderstood boy. But he seems to have problems in finishing his sister off as other people get in the way. He manages to take them out while still looking for that one girl he needs.<br /><br />There have been a lot of those horror movies involving teenagers getting hacked to pieces by a masked or gruesome killer. But this one started it all, sort of. If you think about it, most of those horror movies we all remember are the ones that have Freddy Kruger or Jason chasing around half naked girls. Well, if it wasn't for Halloween, those characters wouldn't have haunted our dreams when we were children.<br /><br />Halloween's director, John Carpenter, got a lot out of the horror movies of the '50s and combined everything he knew into one film that scared the hell out of a lot of people back in the late '70s. This films solidified him as a director to watch and also jump started the career of Jamie Lee Curtis, who plays the girl being stalked by the masked killer.<br /><br />This film may seem clich√© today, but back then there wasn't much out there like this. It's been copied from and ripped off of, but Halloween will always remain the quintessential teenage horror movie. It still gives you chills listening to Carpenter's thrilling music while we see another victim get chased by that shadowy Michael Myers.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIhpJJ8Vh-k6"
      },
      "source": [
        "**1. Data Preparation**\r\n",
        "\r\n",
        "The purpose of the imdb dataset is to predict the sentiment label according to the movie reviews.\r\n",
        "\r\n",
        "There are 20000 text reviews in the training dataset and 5000 in the testing dataset, with half positive and half negative, respectively.\r\n",
        "\r\n",
        "The pre-processing of the text dataset is a little bit complex, which includes word division (for Chinese only, not relevant to this demonstration), dictionary construction, encoding, sequence filling, and data pipeline construction, etc.\r\n",
        "\r\n",
        "There are two popular mothods of text preparation in TensorFlow.\r\n",
        "\r\n",
        "The first one is constructing the text data generator using Tokenizer in tf.keras.preprocessing, together with tf.keras.utils.Sequence.\r\n",
        "\r\n",
        "The second one is using tf.data.Dataset, together with the pre-processing layer tf.keras.layers.experimental.preprocessing.TextVectorization.\r\n",
        "\r\n",
        "The former is more complex and is demonstrated here.\r\n",
        "\r\n",
        "The latter is the original method of TensorFlow, which is simpler.\r\n",
        "\r\n",
        "Below is the introduction to the second method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szxk7yE9kkQp"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import tensorflow as tf\r\n",
        "import re, string"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0BhFYDrkxOy"
      },
      "source": [
        "MAX_WORDS = 10000\r\n",
        "MAX_LEN = 200\r\n",
        "BATCH_SIZE = 20\r\n",
        "\r\n",
        "train_data_path = \"../data/imdb/train.csv\"\r\n",
        "test_data_path = \"../data/imdb/test.csv\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcQO3JYSk_Xw"
      },
      "source": [
        "def split_line(line):\r\n",
        "    arr = tf.strings.split(line, \"\\t\")\r\n",
        "    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]), tf.int32), axis=0)\r\n",
        "    text = tf.expand_dims(arr[1], axis=0)\r\n",
        "    return (text, label)\r\n",
        "\r\n",
        "def clean_text(text):\r\n",
        "    lowercase = tf.strings.lower(text)\r\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\r\n",
        "    clean_punct = tf.strings.regex_replace(stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\")\r\n",
        "    return clean_punct"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq9_DpTaoHUa"
      },
      "source": [
        "ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \\\r\n",
        "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\r\n",
        "   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \\\r\n",
        "   .prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \\\r\n",
        "   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \\\r\n",
        "   .batch(BATCH_SIZE) \\\r\n",
        "   .prefetch(tf.data.experimental.AUTOTUNE)\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zoLo7F5lVIj",
        "outputId": "bfcd0df4-2b26-4164-debe-61b2e5a01249"
      },
      "source": [
        "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\r\n",
        "    max_tokens=MAX_WORDS-1, # leave one item for the placeholder\r\n",
        "    standardize=clean_text,\r\n",
        "    split=\"whitespace\", \r\n",
        "    output_mode='int',\r\n",
        "    output_sequence_length=MAX_LEN \r\n",
        ")\r\n",
        "\r\n",
        "# vectorize \"fit\" on train text\r\n",
        "ds_text = ds_train_raw.map(lambda text, label: text)\r\n",
        "vectorize_layer.adapt(ds_text)\r\n",
        "print(vectorize_layer.get_vocabulary()[:100])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'on', 'not', 'you', 'his', 'are', 'have', 'be', 'he', 'one', 'its', 'at', 'all', 'by', 'an', 'they', 'from', 'who', 'so', 'like', 'her', 'just', 'or', 'about', 'has', 'if', 'out', 'some', 'there', 'what', 'good', 'more', 'when', 'very', 'she', 'even', 'my', 'no', 'would', 'up', 'time', 'only', 'which', 'story', 'really', 'their', 'were', 'had', 'see', 'can', 'me', 'than', 'we', 'much', 'well', 'get', 'been', 'will', 'into', 'people', 'also', 'other', 'do', 'bad', 'because', 'great', 'first', 'how', 'him', 'most', 'dont', 'made', 'then', 'them', 'films', 'movies', 'way', 'make', 'could', 'too', 'any']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m4k__LrsgKB",
        "outputId": "fad465fd-dfee-4728-8ecf-0a8d8f2c506b"
      },
      "source": [
        "sentences = [\"this is sentence 1\",\r\n",
        "             \"this is sentence 2\"]\r\n",
        "\r\n",
        "print(vectorize_layer(sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[  11    7 4309  468    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]\n",
            " [  11    7 4309  286    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]], shape=(2, 200), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU3VccxDsWxb"
      },
      "source": [
        "#Word encoding\r\n",
        "ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \\\r\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAeio7-Zr4Ci"
      },
      "source": [
        "**2. Model Definition**\r\n",
        "\r\n",
        "Usually there are three ways of modeling using APIs of Keras: sequential modeling using Sequential() function, arbitrary modeling using functional API, and customized modeling by inheriting base class Model.\r\n",
        "\r\n",
        "In this example, we use customized modeling by inheriting base class Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW4mhJaatXyp"
      },
      "source": [
        "class CnnModel(tf.keras.Model):\r\n",
        "    def __init__(self):\r\n",
        "        super(CnnModel, self).__init__()\r\n",
        "        \r\n",
        "    def build(self,input_shape):\r\n",
        "        self.embedding = tf.keras.layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)\r\n",
        "        self.conv_1 = tf.keras.layers.Conv1D(16, kernel_size= 5,name = \"conv_1\",activation = \"relu\")\r\n",
        "        self.pool_1 = tf.keras.layers.MaxPool1D(name = \"pool_1\")\r\n",
        "        self.conv_2 = tf.keras.layers.Conv1D(128, kernel_size=2,name = \"conv_2\",activation = \"relu\")\r\n",
        "        self.pool_2 = tf.keras.layers.MaxPool1D(name = \"pool_2\")\r\n",
        "        self.flatten = tf.keras.layers.Flatten()\r\n",
        "        self.dense = tf.keras.layers.Dense(1,activation = \"sigmoid\")\r\n",
        "        super(CnnModel,self).build(input_shape)\r\n",
        "    \r\n",
        "    def call(self, x):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x = self.conv_1(x)\r\n",
        "        x = self.pool_1(x)\r\n",
        "        x = self.conv_2(x)\r\n",
        "        x = self.pool_2(x)\r\n",
        "        x = self.flatten(x)\r\n",
        "        x = self.dense(x)\r\n",
        "        return(x)\r\n",
        "    \r\n",
        "    # To show Output Shape\r\n",
        "    def summary(self):\r\n",
        "        x_input = tf.keras.layers.Input(shape = (MAX_LEN,))\r\n",
        "        output = self.call(x_input)\r\n",
        "        model = tf.keras.Model(inputs = x_input,outputs = output)\r\n",
        "        model.summary()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCcNiP9quPUc",
        "outputId": "34b3e99a-584f-48dd-c49a-8e8ce58aa486"
      },
      "source": [
        "model = CnnModel()\r\n",
        "model.build(input_shape =(None,MAX_LEN))\r\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 200)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 200, 7)            70000     \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv1D)              (None, 196, 16)           576       \n",
            "_________________________________________________________________\n",
            "pool_1 (MaxPooling1D)        (None, 98, 16)            0         \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv1D)              (None, 97, 128)           4224      \n",
            "_________________________________________________________________\n",
            "pool_2 (MaxPooling1D)        (None, 48, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6144)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6145      \n",
            "=================================================================\n",
            "Total params: 80,945\n",
            "Trainable params: 80,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dNZUIbcvgaj"
      },
      "source": [
        "**3. Model Training**\r\n",
        "\r\n",
        "There are three usual ways for model training: use internal function fit, use internal function train_on_batch, and customized training loop. Here we use the customized training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evca5t_rxZ1a"
      },
      "source": [
        "#Time Stamp\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    ts = tf.timestamp()\r\n",
        "    today_ts = tf.timestamp()%(23*60*60)\r\n",
        "\r\n",
        "    hour = tf.cast(today_ts//3600+8, tf.int32)%tf.constant(24)\r\n",
        "    minute = tf.cast((today_ts%3600)//60, tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60), tf.int32)\r\n",
        "\r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\", m)) == 1:\r\n",
        "            return (tf.strings.format(\"0{}\", m))\r\n",
        "        else:\r\n",
        "            return (tf.strings.format(\"{}\", m))\r\n",
        "\r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minute),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8+timestring)\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCxbmX6O4rnC"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Nadam()\r\n",
        "loss_func = tf.keras.losses.BinaryCrossentropy()\r\n",
        "\r\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n",
        "train_metric = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\r\n",
        "\r\n",
        "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\r\n",
        "valid_metric = tf.keras.metrics.BinaryAccuracy(name='valid_accuracy')\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def train_step(model, features, labels):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        predictions = model(features, training=True)\r\n",
        "        loss = loss_func(labels, predictions)\r\n",
        "    \r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "    train_loss.update_state(loss)\r\n",
        "    train_metric.update_state(labels, predictions)\r\n",
        "\r\n",
        "@tf.function\r\n",
        "def valid_step(model, features, labels):\r\n",
        "    predictions = model(features, training=False)\r\n",
        "    batch_loss = loss_func(labels, predictions)\r\n",
        "    valid_loss.update_state(batch_loss)\r\n",
        "    valid_metric.update_state(labels, predictions)\r\n",
        "\r\n",
        "def train_model(model, ds_train, ds_valid, epochs):\r\n",
        "    for epoch in tf.range(1, epochs+1):\r\n",
        "\r\n",
        "        for features, labels in ds_train:\r\n",
        "            train_step(model, features, labels)\r\n",
        "\r\n",
        "        for features, labels in ds_valid:\r\n",
        "            valid_step(model, features, labels)\r\n",
        "        \r\n",
        "        logs = \"Epoch={}, Loss:{}, Accuracy:{}, Valid_loss: {}, Valid_accuracy:{}\"\r\n",
        "\r\n",
        "        if epoch%1==0:\r\n",
        "            printbar()\r\n",
        "            tf.print(tf.strings.format(logs,\r\n",
        "                                       (epoch, train_loss.result(), train_metric.result(), valid_loss.result(), valid_metric.result())))\r\n",
        "            tf.print(\"\")\r\n",
        "\r\n",
        "\r\n",
        "        train_loss.reset_states()\r\n",
        "        valid_loss.reset_states()\r\n",
        "        train_metric.reset_states()\r\n",
        "        valid_metric.reset_states()\r\n",
        "        "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isohs-rd9VGU",
        "outputId": "e3f83c77-9da7-4b65-88b1-259d53a4dc8f"
      },
      "source": [
        "train_model(model, ds_train, ds_test, epochs=6)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================06:09:00\n",
            "Epoch=1, Loss:0.137153059, Accuracy:0.94915, Valid_loss: 0.402908385, Valid_accuracy:0.8674\n",
            "\n",
            "================================================================================06:09:07\n",
            "Epoch=2, Loss:0.0935169086, Accuracy:0.9676, Valid_loss: 0.541632116, Valid_accuracy:0.863\n",
            "\n",
            "================================================================================06:09:13\n",
            "Epoch=3, Loss:0.0567635931, Accuracy:0.9803, Valid_loss: 0.758906305, Valid_accuracy:0.8482\n",
            "\n",
            "================================================================================06:09:20\n",
            "Epoch=4, Loss:0.0327552296, Accuracy:0.98975, Valid_loss: 0.860463619, Valid_accuracy:0.8538\n",
            "\n",
            "================================================================================06:09:27\n",
            "Epoch=5, Loss:0.0190788414, Accuracy:0.99385, Valid_loss: 1.03909099, Valid_accuracy:0.8528\n",
            "\n",
            "================================================================================06:09:33\n",
            "Epoch=6, Loss:0.0168200135, Accuracy:0.99355, Valid_loss: 1.14302337, Valid_accuracy:0.8542\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvOr9-WA9cwz"
      },
      "source": [
        "**4. Model Evaluation**\r\n",
        "\r\n",
        "The model trained by the customized looping is not compiled, so the method `model.evaluate(ds_valid)` cant be applied directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG_t1H0s-8es"
      },
      "source": [
        "def evaluate_model(model,ds_valid):\r\n",
        "    for features, labels in ds_valid:\r\n",
        "         valid_step(model,features,labels)\r\n",
        "    logs = 'Valid Loss:{},Valid Accuracy:{}' \r\n",
        "    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))\r\n",
        "    \r\n",
        "    valid_loss.reset_states()\r\n",
        "    train_metric.reset_states()\r\n",
        "    valid_metric.reset_states()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OyFGhxM_tAm",
        "outputId": "74acac1e-a0c3-4d84-9b5a-1962c5816f68"
      },
      "source": [
        "evaluate_model(model, ds_test)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Valid Loss:1.14302325,Valid Accuracy:0.8542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkjeTaAZ_5QD"
      },
      "source": [
        "**5. Model Saving**\r\n",
        "\r\n",
        "Model saving with the original wy of TensorFlow is recommended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szzSnZP3Ab9r",
        "outputId": "5b2593d4-0a5a-4389-a507-87496d863936"
      },
      "source": [
        "%cd \"/gdrive/MyDrive/Colab Notebooks/git/eat_tensorflow2_in_30_days/notebooks\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/git/eat_tensorflow2_in_30_days/notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il48S0EHAQHb",
        "outputId": "55e55c88-c320-4708-d1dc-37b2d0231c5d"
      },
      "source": [
        "model.save(\"../model_weights/imdb_model\", save_format=\"tf\")\r\n",
        "print('export saved model')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ../model_weights/imdb_model/assets\n",
            "export saved model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OYJSXkLAo0L",
        "outputId": "9192e3b6-c9b3-4be4-e353-abd935b97251"
      },
      "source": [
        "model_loaded = tf.keras.models.load_model(\"../model_weights/imdb_model\")\r\n",
        "evaluate_model(model_loaded, ds_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Valid Loss:1.01482117,Valid Accuracy:0.8542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CTZefPBA3uz",
        "outputId": "2d7dff53-1489-4fcb-a95a-ead8fdfb89a4"
      },
      "source": [
        "%cd \"../model_weights/\"\r\n",
        "!rm -r imdb_model"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/MyDrive/Colab Notebooks/git/eat_tensorflow2_in_30_days/model_weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvck2y6-BHkE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}