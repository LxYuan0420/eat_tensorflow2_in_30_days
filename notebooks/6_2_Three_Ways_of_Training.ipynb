{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-2 Three Ways of Training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOoVbc11ebUqQaGSPeb9HV2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/6_2_Three_Ways_of_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfBwffgaowGB"
      },
      "source": [
        "**6-2 Three Ways of Training**\r\n",
        "\r\n",
        "There are three ways of model training: using pre-defined fit method, using pre-defined tran_on_batch method, using customized training loop.\r\n",
        "\r\n",
        "Note: fit_generator method is not recommended in tf.keras since it has been merged into fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH6pbEDApcSd"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import * \r\n",
        "\r\n",
        "# Time stamps\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    ts = tf.timestamp()\r\n",
        "    today_ts = ts%(24*60*60)\r\n",
        "\r\n",
        "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\r\n",
        "    minite = tf.cast((today_ts%3600)//60,tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\r\n",
        "    \r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\r\n",
        "            return(tf.strings.format(\"0{}\",m))\r\n",
        "        else:\r\n",
        "            return(tf.strings.format(\"{}\",m))\r\n",
        "    \r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8,end = \"\")\r\n",
        "    tf.print(timestring)\r\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8XJ0XOZpdn_",
        "outputId": "3ce190bc-2c7d-4882-d060-d394ccb02837"
      },
      "source": [
        "MAX_LEN = 300\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "(x_train, y_train), (x_test, y_test) = datasets.reuters.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfJBLTnwpnkV",
        "outputId": "4d1b4ef0-d730-4b09-defb-752eaf1aa6c9"
      },
      "source": [
        "x_train[:2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]),\n",
              "       list([1, 3267, 699, 3434, 2295, 56, 16784, 7511, 9, 56, 3906, 1073, 81, 5, 1198, 57, 366, 737, 132, 20, 4093, 7, 19261, 49, 2295, 13415, 1037, 3267, 699, 3434, 8, 7, 10, 241, 16, 855, 129, 231, 783, 5, 4, 587, 2295, 13415, 30625, 775, 7, 48, 34, 191, 44, 35, 1795, 505, 17, 12])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWVayDCPsA8q",
        "outputId": "266ec453-ac27-4c2c-9d1d-8d8fbe6fc951"
      },
      "source": [
        "y_train[:2]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmmWCH69pqX-"
      },
      "source": [
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_LEN)\r\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_LEN)\r\n",
        "\r\n",
        "MAX_WORDS = x_train.max() + 1\r\n",
        "CAT_NUM = y_train.max() + 1\r\n",
        "\r\n",
        "# we are trying differents way of traininig so we dont do repeat()\r\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE).cache()\r\n",
        "ds_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE).cache()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhkjYJ5qhNZ"
      },
      "source": [
        "**1. Predefined `fit` method**\r\n",
        "\r\n",
        "This is a powerful method, which supports training the data with types of numpy array. `tf.data.Dataset` and Python generator.\r\n",
        "\r\n",
        "This method also supports complicated logical controlling through proper configuration of the callbacks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCJ5zbUq8zi"
      },
      "source": [
        "def create_model():\r\n",
        "\r\n",
        "    model = models.Sequential()\r\n",
        "    model.add(layers.Embedding(MAX_WORDS, 7, input_length=MAX_LEN))\r\n",
        "    model.add(layers.Conv1D(filters=64, kernel_size=5, activation='relu'))\r\n",
        "    model.add(layers.MaxPool1D())\r\n",
        "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu'))\r\n",
        "    model.add(layers.MaxPool1D())\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(CAT_NUM, activation='softmax'))\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "def compile_model(model):\r\n",
        "    model.compile(\r\n",
        "        optimizer=optimizers.Nadam(),\r\n",
        "        loss = losses.SparseCategoricalCrossentropy(),\r\n",
        "        metrics = [metrics.SparseCategoricalAccuracy(), metrics.SparseTopKCategoricalAccuracy()]\r\n",
        "    )\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqupHZwQseKN",
        "outputId": "32764fb7-ee3e-4cb8-9c58-7bdf38d9e457"
      },
      "source": [
        "model = create_model()\r\n",
        "model.summary()\r\n",
        "model = compile_model(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ-sha67su4l",
        "outputId": "1fbf9770-1a67-44a3-b72e-51515e5a128f"
      },
      "source": [
        "history = model.fit(\r\n",
        "    ds_train,\r\n",
        "    validation_data=ds_test,\r\n",
        "    epochs=10\r\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "281/281 [==============================] - 10s 30ms/step - loss: 2.4155 - sparse_categorical_accuracy: 0.3673 - sparse_top_k_categorical_accuracy: 0.7121 - val_loss: 1.7154 - val_sparse_categorical_accuracy: 0.5521 - val_sparse_top_k_categorical_accuracy: 0.7631\n",
            "Epoch 2/10\n",
            "281/281 [==============================] - 8s 29ms/step - loss: 1.5836 - sparse_categorical_accuracy: 0.5895 - sparse_top_k_categorical_accuracy: 0.7820 - val_loss: 1.5184 - val_sparse_categorical_accuracy: 0.6260 - val_sparse_top_k_categorical_accuracy: 0.7970\n",
            "Epoch 3/10\n",
            "281/281 [==============================] - 8s 30ms/step - loss: 1.2228 - sparse_categorical_accuracy: 0.6768 - sparse_top_k_categorical_accuracy: 0.8492 - val_loss: 1.5260 - val_sparse_categorical_accuracy: 0.6483 - val_sparse_top_k_categorical_accuracy: 0.8103\n",
            "Epoch 4/10\n",
            "281/281 [==============================] - 8s 29ms/step - loss: 0.9419 - sparse_categorical_accuracy: 0.7539 - sparse_top_k_categorical_accuracy: 0.9095 - val_loss: 1.6710 - val_sparse_categorical_accuracy: 0.6492 - val_sparse_top_k_categorical_accuracy: 0.8121\n",
            "Epoch 5/10\n",
            "281/281 [==============================] - 8s 29ms/step - loss: 0.6951 - sparse_categorical_accuracy: 0.8294 - sparse_top_k_categorical_accuracy: 0.9477 - val_loss: 1.8917 - val_sparse_categorical_accuracy: 0.6500 - val_sparse_top_k_categorical_accuracy: 0.8161\n",
            "Epoch 6/10\n",
            "281/281 [==============================] - 8s 29ms/step - loss: 0.5191 - sparse_categorical_accuracy: 0.8782 - sparse_top_k_categorical_accuracy: 0.9695 - val_loss: 2.1520 - val_sparse_categorical_accuracy: 0.6420 - val_sparse_top_k_categorical_accuracy: 0.8157\n",
            "Epoch 7/10\n",
            "281/281 [==============================] - 8s 29ms/step - loss: 0.4120 - sparse_categorical_accuracy: 0.9067 - sparse_top_k_categorical_accuracy: 0.9825 - val_loss: 2.3538 - val_sparse_categorical_accuracy: 0.6438 - val_sparse_top_k_categorical_accuracy: 0.8121\n",
            "Epoch 8/10\n",
            "281/281 [==============================] - 8s 29ms/step - loss: 0.3438 - sparse_categorical_accuracy: 0.9208 - sparse_top_k_categorical_accuracy: 0.9883 - val_loss: 2.5871 - val_sparse_categorical_accuracy: 0.6402 - val_sparse_top_k_categorical_accuracy: 0.8108\n",
            "Epoch 9/10\n",
            "281/281 [==============================] - 8s 28ms/step - loss: 0.2972 - sparse_categorical_accuracy: 0.9322 - sparse_top_k_categorical_accuracy: 0.9910 - val_loss: 2.7657 - val_sparse_categorical_accuracy: 0.6313 - val_sparse_top_k_categorical_accuracy: 0.8179\n",
            "Epoch 10/10\n",
            "281/281 [==============================] - 8s 28ms/step - loss: 0.2635 - sparse_categorical_accuracy: 0.9376 - sparse_top_k_categorical_accuracy: 0.9933 - val_loss: 2.9149 - val_sparse_categorical_accuracy: 0.6264 - val_sparse_top_k_categorical_accuracy: 0.8188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4YZymkHs6Cs"
      },
      "source": [
        "**2. Pre-defined train_on_batch method**\r\n",
        "\r\n",
        "This pre-defined method allows fine-controlling to the training procedure for each batch without the callbacks, which is even more flexible than fit method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsX5Tbkgs_tj",
        "outputId": "a391ea31-e768-4979-9e2c-b3f2459c2887"
      },
      "source": [
        "model_2 = create_model()\r\n",
        "model_2.summary()\r\n",
        "model_2 = compile_model(model_2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwQF-3rNtHB8"
      },
      "source": [
        "def train_model(model, ds_train, ds_valid, epoches):\r\n",
        "    for epoch in tf.range(1, epoches+1):\r\n",
        "        model.reset_metrics()\r\n",
        "\r\n",
        "        #Reduce learnign rate at the late stage of training.\r\n",
        "        if epoch == 5:\r\n",
        "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\r\n",
        "            tf.print(f\"Lowering optimizer learnign rate to: {model.optimizer.lr}\")\r\n",
        "            \r\n",
        "        for x, y in ds_train:\r\n",
        "            train_result = model.train_on_batch(x, y)\r\n",
        "\r\n",
        "        for x, y in ds_valid:\r\n",
        "            valid_result = model.test_on_batch(x, y, reset_metrics=False)\r\n",
        "\r\n",
        "        if epoch%1 == 0:\r\n",
        "            printbar()\r\n",
        "            tf.print(f\"epoch = {epoch}\")\r\n",
        "            print(\"Train: \", dict(zip(model.metrics_names, train_result)))\r\n",
        "            print(\"Valid: \", dict(zip(model.metrics_names, valid_result)))\r\n",
        "            print(\"\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmmI_FyFuQn_",
        "outputId": "5aa86da4-7eac-4dd6-8ead-ea0bbaab596b"
      },
      "source": [
        "train_model(model_2, ds_train, ds_test, 10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================00:15:08\n",
            "epoch = 1\n",
            "Train:  {'loss': 1.3428311347961426, 'sparse_categorical_accuracy': 0.6363636255264282, 'sparse_top_k_categorical_accuracy': 0.8636363744735718}\n",
            "Valid:  {'loss': 1.8067418336868286, 'sparse_categorical_accuracy': 0.5209261178970337, 'sparse_top_k_categorical_accuracy': 0.7595725655555725}\n",
            "\n",
            "================================================================================00:15:17\n",
            "epoch = 2\n",
            "Train:  {'loss': 0.9232032895088196, 'sparse_categorical_accuracy': 0.7272727489471436, 'sparse_top_k_categorical_accuracy': 0.9090909361839294}\n",
            "Valid:  {'loss': 1.5674611330032349, 'sparse_categorical_accuracy': 0.6073018908500671, 'sparse_top_k_categorical_accuracy': 0.7858415246009827}\n",
            "\n",
            "================================================================================00:15:26\n",
            "epoch = 3\n",
            "Train:  {'loss': 0.5672282576560974, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 0.9545454382896423}\n",
            "Valid:  {'loss': 1.5064771175384521, 'sparse_categorical_accuracy': 0.645146906375885, 'sparse_top_k_categorical_accuracy': 0.8072128295898438}\n",
            "\n",
            "================================================================================00:15:36\n",
            "epoch = 4\n",
            "Train:  {'loss': 0.3684699237346649, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 0.9545454382896423}\n",
            "Valid:  {'loss': 1.640152096748352, 'sparse_categorical_accuracy': 0.6504897475242615, 'sparse_top_k_categorical_accuracy': 0.8103294968605042}\n",
            "\n",
            "Lowering optimizer learnign rate to: <tf.Variable 'Nadam/learning_rate:0' shape=() dtype=float32, numpy=0.0005>\n",
            "================================================================================00:15:45\n",
            "epoch = 5\n",
            "Train:  {'loss': 0.23268045485019684, 'sparse_categorical_accuracy': 0.9090909361839294, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "Valid:  {'loss': 1.8441411256790161, 'sparse_categorical_accuracy': 0.642920732498169, 'sparse_top_k_categorical_accuracy': 0.8040961623191833}\n",
            "\n",
            "================================================================================00:15:54\n",
            "epoch = 6\n",
            "Train:  {'loss': 0.18020135164260864, 'sparse_categorical_accuracy': 0.9545454382896423, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "Valid:  {'loss': 2.00482177734375, 'sparse_categorical_accuracy': 0.6371327042579651, 'sparse_top_k_categorical_accuracy': 0.8085485100746155}\n",
            "\n",
            "================================================================================00:16:03\n",
            "epoch = 7\n",
            "Train:  {'loss': 0.1270902454853058, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "Valid:  {'loss': 2.1660194396972656, 'sparse_categorical_accuracy': 0.6357969641685486, 'sparse_top_k_categorical_accuracy': 0.8112199306488037}\n",
            "\n",
            "================================================================================00:16:13\n",
            "epoch = 8\n",
            "Train:  {'loss': 0.0824228972196579, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "Valid:  {'loss': 2.3201332092285156, 'sparse_categorical_accuracy': 0.6326802968978882, 'sparse_top_k_categorical_accuracy': 0.8089937567710876}\n",
            "\n",
            "================================================================================00:16:22\n",
            "epoch = 9\n",
            "Train:  {'loss': 0.054936524480581284, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "Valid:  {'loss': 2.4750399589538574, 'sparse_categorical_accuracy': 0.6300088763237, 'sparse_top_k_categorical_accuracy': 0.8107746839523315}\n",
            "\n",
            "================================================================================00:16:32\n",
            "epoch = 10\n",
            "Train:  {'loss': 0.0404348224401474, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}\n",
            "Valid:  {'loss': 2.622816562652588, 'sparse_categorical_accuracy': 0.6291184425354004, 'sparse_top_k_categorical_accuracy': 0.8103294968605042}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhYh2USLuWbj"
      },
      "source": [
        "**3. Customized Training Loop**\r\n",
        "\r\n",
        "Re-compilation of the model is not required in the customized training loop, just back-propagate the iterative parameters through the optimizer according to the loss function, which gives us the highest flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDk9LQTBukZq",
        "outputId": "0ddd3635-4a28-4d94-8fab-ee344e62e5b5"
      },
      "source": [
        "model_3 = create_model()\r\n",
        "model_3.summary()\r\n",
        "model_3 = compile_model(model_3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 300, 7)            216874    \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 296, 64)           2304      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 148, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 146, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_9 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2336)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 46)                107502    \n",
            "=================================================================\n",
            "Total params: 332,856\n",
            "Trainable params: 332,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_mROHbburi6"
      },
      "source": [
        "optimizer = optimizers.Nadam()\r\n",
        "loss_func = losses.SparseCategoricalCrossentropy()\r\n",
        "\r\n",
        "train_loss = metrics.Mean()\r\n",
        "train_metric = metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\r\n",
        "\r\n",
        "valid_loss = metrics.Mean()\r\n",
        "valid_metric = metrics.SparseCategoricalAccuracy(name=\"valid_accuracy\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML0wpq_vvV1b"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(model, features, labels):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        predictions = model(features)\r\n",
        "        loss = loss_func(labels, predictions)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "    train_loss.update_state(loss)\r\n",
        "    train_metric.update_state(labels, predictions)\r\n",
        "\r\n",
        "def valid_step(model, features, labels):\r\n",
        "    predictions = model(features)\r\n",
        "    loss = loss_func(labels, predictions)\r\n",
        "    valid_loss.update_state(loss)\r\n",
        "    valid_metric.update_state(labels, predictions)\r\n",
        "\r\n",
        "def train_model(model, ds_train, ds_test, epoches):\r\n",
        "\r\n",
        "    for epoch in tf.range(1, epoches+1):\r\n",
        "\r\n",
        "        for features, labels in ds_train:\r\n",
        "            train_step(model, features, labels)\r\n",
        "\r\n",
        "        for features, labels in ds_train:\r\n",
        "            valid_step(model, features, labels)\r\n",
        "\r\n",
        "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\r\n",
        "        \r\n",
        "        if epoch%1 ==0:\r\n",
        "            printbar()\r\n",
        "            tf.print(tf.strings.format(logs,\r\n",
        "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\r\n",
        "            tf.print(\"\")\r\n",
        "\r\n",
        "        #for next epoch, we reset the metrics\r\n",
        "        train_loss.reset_states()\r\n",
        "        train_metric.reset_states()\r\n",
        "        valid_loss.reset_states()\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgHiURDpxABs",
        "outputId": "6f943c53-2cbe-463d-c7c8-2c31ac6fab31"
      },
      "source": [
        "train_model(model_3, ds_train, ds_test, 10)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================00:27:07\n",
            "Epoch=1,Loss:2.02902699,Accuracy:0.468269885,Valid Loss:1.6026293,Valid Accuracy:0.585393\n",
            "\n",
            "================================================================================00:27:18\n",
            "Epoch=2,Loss:1.48710024,Accuracy:0.612781107,Valid Loss:1.23912287,Valid Accuracy:0.627866864\n",
            "\n",
            "================================================================================00:27:29\n",
            "Epoch=3,Loss:1.19473851,Accuracy:0.685370743,Valid Loss:0.92709285,Valid Accuracy:0.670637548\n",
            "\n",
            "================================================================================00:27:40\n",
            "Epoch=4,Loss:0.935973227,Accuracy:0.752282321,Valid Loss:0.664993882,Valid Accuracy:0.711311519\n",
            "\n",
            "================================================================================00:27:51\n",
            "Epoch=5,Loss:0.708897114,Accuracy:0.814963281,Valid Loss:0.490126163,Valid Accuracy:0.745090187\n",
            "\n",
            "================================================================================00:28:02\n",
            "Epoch=6,Loss:0.539092541,Accuracy:0.866288126,Valid Loss:0.385754317,Valid Accuracy:0.771914184\n",
            "\n",
            "================================================================================00:28:13\n",
            "Epoch=7,Loss:0.43475759,Accuracy:0.894232929,Valid Loss:0.329027325,Valid Accuracy:0.792648792\n",
            "\n",
            "================================================================================00:28:24\n",
            "Epoch=8,Loss:0.365386456,Accuracy:0.913827658,Valid Loss:0.292475045,Valid Accuracy:0.809340894\n",
            "\n",
            "================================================================================00:28:35\n",
            "Epoch=9,Loss:0.317523807,Accuracy:0.924849689,Valid Loss:0.266854852,Valid Accuracy:0.82286793\n",
            "\n",
            "================================================================================00:28:46\n",
            "Epoch=10,Loss:0.282765359,Accuracy:0.932086408,Valid Loss:0.249779209,Valid Accuracy:0.833967924\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "775MezGGxClB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}