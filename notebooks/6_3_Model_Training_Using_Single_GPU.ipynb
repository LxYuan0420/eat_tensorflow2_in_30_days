{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6-3 Model Training Using Single GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPykgimdNRL7R3kHcx7OA4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LxYuan0420/eat_tensorflow2_in_30_days/blob/master/notebooks/6_3_Model_Training_Using_Single_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj3obtXV18vV"
      },
      "source": [
        "The training procedure of deep learning is usually time consuming. It even takes tens of days for training, and there is no need to mention those take days or hours.\r\n",
        "\r\n",
        "The time is mainly consumpted by two stages, data preparation and parameter iteration.\r\n",
        "\r\n",
        "We can increase the number of process to alleviate this issue if data preparation takes the majority of time.\r\n",
        "\r\n",
        "However, if the majority of time is taken by parameter iteration, we need to use GPU or Google TPU for acceleration.\r\n",
        "\r\n",
        "You may refer to this article for further details: \"GPU acceleration for Keras Models - How to Use Free Colab GPUs (in Chinese)\"\r\n",
        "\r\n",
        "There is no need to modify source code for switching from CPU to GPU when using the pre-defined fit method or the customized training loops. When GPU is available and the device is not specified, TensorFlow automatically chooses GPU for tensor creating and computing.\r\n",
        "\r\n",
        "However, for the case of using shared GPU with multiple users, sucha as using server of the company or the lab, we need to add following code to specify the GPU ID and the GPU memory size that we are going to use, in order to avoid the GPU resources to be occupied by a single user (actually TensorFlow acquires all GPU cors and all GPU memories by default) and allows multiple users perform training on it.\r\n",
        "\r\n",
        "In Colab notebook, choose GPU in Edit -> Notebook Settings -> Hardware Accelerator\r\n",
        "\r\n",
        "Note: the following code only executes on Colab.\r\n",
        "\r\n",
        "You may use the following link for testing (tf_singleGPU, in Chinese)\r\n",
        "\r\n",
        "https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "snOoVIfq4pm2",
        "outputId": "c38d2080-86a1-467c-f9c1-e5354c790298"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5l_2Ecm4x8b"
      },
      "source": [
        "from tensorflow.keras import * \r\n",
        "\r\n",
        "# Time stamp\r\n",
        "@tf.function\r\n",
        "def printbar():\r\n",
        "    ts = tf.timestamp()\r\n",
        "    today_ts = ts%(24*60*60)\r\n",
        "\r\n",
        "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\r\n",
        "    minite = tf.cast((today_ts%3600)//60,tf.int32)\r\n",
        "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\r\n",
        "    \r\n",
        "    def timeformat(m):\r\n",
        "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\r\n",
        "            return(tf.strings.format(\"0{}\",m))\r\n",
        "        else:\r\n",
        "            return(tf.strings.format(\"{}\",m))\r\n",
        "    \r\n",
        "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\r\n",
        "                timeformat(second)],separator = \":\")\r\n",
        "    tf.print(\"==========\"*8,end = \"\")\r\n",
        "    tf.print(timestring)\r\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn6rpDwd40RS"
      },
      "source": [
        "**1. GPU Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoNSEj9g44CB"
      },
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\r\n",
        "\r\n",
        "if gpus:\r\n",
        "    gpu0 = gpus[0]\r\n",
        "    tf.config.experimental.set_memory_growth(gpu0, True)\r\n",
        "    tf.config.set_visible_devices([gpu0], \"GPU\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAJ250mQ5Hih"
      },
      "source": [
        "Compare the computing speed between GPU and CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWZq033l5OjJ",
        "outputId": "2363fe88-566f-41a0-f4e9-87eeb69092c4"
      },
      "source": [
        "printbar()\r\n",
        "\r\n",
        "with tf.device(\"/gpu:0\"):\r\n",
        "    tf.random.set_seed(0)\r\n",
        "    a = tf.random.uniform((10000, 100), minval=0, maxval=3.0)\r\n",
        "    b = tf.random.uniform((100, 10000), minval=0, maxval=3.0)\r\n",
        "    c = a@b\r\n",
        "    tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=0), axis=0))\r\n",
        "printbar()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================15:03:03\n",
            "2.25052979e+10\n",
            "================================================================================15:03:04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDKekDFB5mnd",
        "outputId": "d066a459-fce7-4066-9552-04d004eee58e"
      },
      "source": [
        "printbar()\r\n",
        "with tf.device(\"/cpu:0\"):\r\n",
        "    tf.random.set_seed(0)\r\n",
        "    a = tf.random.uniform((10000, 100), minval=0, maxval=3.0)\r\n",
        "    b = tf.random.uniform((100, 10000), minval=0, maxval=3.0)\r\n",
        "    c = a@b\r\n",
        "    tf.print(tf.reduce_sum(tf.reduce_sum(c, axis=0), axis=0))\r\n",
        "printbar()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================15:04:15\n",
            "2.25052979e+10\n",
            "================================================================================15:04:16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-raWW9455id"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}